{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimential Scoring Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Previous Package + Reading in the Model Fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from Shallow_ML_Models.DataPreprocessor import TextPreprocessor\n",
    "from Shallow_ML_Models.DataPreprocessor import TextPreprocessor_withStem\n",
    "import pickle\n",
    "lr_model = pickle.load(open(r'Shallow_ML_Models/LR_model_withStem.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from LSTM_Model.utils import labelencoder, predict_unseen_test\n",
    "from LSTM_Model.load_data import Test_DataProcessor, create_data_loader\n",
    "from LSTM_Model.LSTM_model import BERTbiLSTM\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model_weight = 'ProsusAI/finbert'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_weight)    #tokenizer to use\n",
    "bert_model = BertModel.from_pretrained(bert_model_weight)    #loading base bert model\n",
    "\n",
    "HIDDEN_DIM = 256    ## Model params for FinBERT-biLSTM_2.pt for FinBERT-biLSTM.pt change HIDDEN_DIM TO 128 and N_LAYERS TO 1\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "lstm_model = BERTbiLSTM(bert_model, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "lstm_model.load_state_dict(torch.load(r'LSTM_Model/FinBERT-biLSTM_2.pt', map_location=torch.device('cpu')))\n",
    "lstm_model.to(device)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Table for Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year12 = []\n",
    "period12 = []\n",
    "\n",
    "i = 2013\n",
    "for j in range(3, 12):\n",
    "    year12.append(i)\n",
    "    period12.append(j+1)\n",
    "\n",
    "for i in range(2014, 2021):\n",
    "    for j in range(0, 12):\n",
    "        year12.append(i)\n",
    "        period12.append(j+1)\n",
    "\n",
    "i = 2021\n",
    "for j in range(0, 9):\n",
    "    year12.append(i)\n",
    "    period12.append(j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = ['positive', 'neutral', 'negative', 'exception']\n",
    "sentiment_score_lr = pd.DataFrame(np.zeros((len(year12), len(column_name))), index = [year12, period12], columns = column_name).reset_index().rename(columns={\"level_0\": \"year\", \"level_1\": \"month\"}).set_index(['year','month'])\n",
    "sentiment_score_lstm = pd.DataFrame(np.zeros((len(year12), len(column_name))), index = [year12, period12], columns = column_name).reset_index().rename(columns={\"level_0\": \"year\", \"level_1\": \"month\"}).set_index(['year','month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the List of files + Reading in the Text + Sentiment Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import sys, time\n",
    "\n",
    "READ_FREQ = 20\n",
    "SAVING_FREQ = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GEDLT progress: 1 / 3101 ; Time Elasped: 0.52 minutes.lasped: 0.52 minutes.\n",
      "gdelt_list_counter = 2 / 3101 ; doc_count = 200 / 524 ; time elasped: 1.12 minutes."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GEDLT progress: 2 / 3101 ; Time Elasped: 1.42 minutes.lasped: 1.42 minutes.\n",
      "Reading GEDLT progress: 3 / 3101 ; Time Elasped: 2.42 minutes.lasped: 2.42 minutes.\n",
      "Reading GEDLT progress: 4 / 3101 ; Time Elasped: 3.02 minutes.lasped: 3.02 minutes.\n",
      "Reading GEDLT progress: 5 / 3101 ; Time Elasped: 3.5 minutes.elasped: 3.5 minutes..\n"
     ]
    }
   ],
   "source": [
    "gdelt_list = pd.read_csv(r\"Data/GDELT/gdelt.csv\", encoding= 'unicode_escape').set_index('filename')\n",
    "gdelt_colName = pd.read_csv(r\"Data/GDELT/CSV.header.dailyupdates.txt\", sep='\\t', header=None).iloc[0].to_list()\n",
    "sentiment_list = [\"positive\", \"neutral\", \"negative\"]\n",
    "start_time = time.time()\n",
    "\n",
    "for gdelt_list_counter in range(0, len(gdelt_list)):\n",
    "    \n",
    "    zip_file_url = urlopen(gdelt_list.iloc[gdelt_list_counter]['hyperlink'])\n",
    "    zip_file = ZipFile(BytesIO(zip_file_url.read()))\n",
    "    document_list = pd.read_csv(zip_file.open(zip_file.namelist()[0]), sep ='\\t', header=None, names=gdelt_colName).set_index(['GLOBALEVENTID'])\n",
    "    US_document_list = document_list[(document_list['Actor1Geo_ADM1Code'] == \"US\") & (document_list['Actor2Geo_ADM1Code'] == \"US\") & (document_list['ActionGeo_ADM1Code'] == \"US\") & ((document_list['Actor1Code'] == \"USA\") | (document_list['Actor2Code'] == \"USA\"))]\n",
    "    text_matrix = pd.DataFrame()\n",
    "    \n",
    "    for doc_count in range(0,len(US_document_list)):\n",
    "        \n",
    "        if (doc_count % READ_FREQ == 0):\n",
    "            news_year = int(str(US_document_list.iloc[doc_count, 0])[0:4])\n",
    "            news_month = int(str(US_document_list.iloc[doc_count, 0])[4:6])\n",
    "            news_date = int(str(US_document_list.iloc[doc_count, 0])[6:8])\n",
    "            news_url = document_list.iloc[doc_count, -1]\n",
    "            exception_count = 0\n",
    "            \n",
    "            try:\n",
    "                soup = BeautifulSoup(urlopen(news_url).read(), features=\"html.parser\")\n",
    "                for script in soup([\"script\", \"style\"]):    # kill all script and style elements\n",
    "                    script.decompose()\n",
    "                text = soup.get_text()    # get text\n",
    "                lines = (line.strip() for line in text.splitlines())    # break into lines and remove leading and trailing space on each\n",
    "                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))    # break multi-headlines into a line each\n",
    "                text = '\\n'.join(chunk for chunk in chunks if chunk)    # drop blank lines\n",
    "                text_matrix = text_matrix.append({'sentence': text}, ignore_index=True)\n",
    "            except:\n",
    "                exception_count += 1\n",
    "        \n",
    "        current_time = time.time()\n",
    "        sys.stdout.write(\"\\rgdelt_list_counter = %s / %s ; doc_count = %s / %s ; time elasped: %s minutes.\" % (gdelt_list_counter + 1, len(gdelt_list), doc_count + 1, len(US_document_list), round((current_time-start_time)/60,2)))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    text_matrix['sentiment_lr'] = lr_model.predict(text_matrix['sentence'])\n",
    "    text_matrix['sentiment_lstm']  = predict_unseen_test(lstm_model, create_data_loader(text_matrix, bert_tokenizer, MAX_LEN, BATCH_SIZE, True), device)['prediction']\n",
    "    \n",
    "    sentiment_score_lr.loc[(news_year, news_month), 'exception'] += exception_count\n",
    "    sentiment_score_lstm.loc[(news_year, news_month), 'exception'] += exception_count\n",
    "    for i in sentiment_list:\n",
    "        sentiment_score_lr.loc[(news_year, news_month), i] += sum(text_matrix['sentiment_lr'] == i)\n",
    "        sentiment_score_lstm.loc[(news_year, news_month), i] += sum(text_matrix['sentiment_lstm'] == sentiment_list.index(i))\n",
    "    \n",
    "    print(\"\\rReading GEDLT progress: %s / %s ; Time Elasped: %s minutes.\" % (gdelt_list_counter + 1, len(gdelt_list), round((current_time-start_time)/60,2)))\n",
    "    if (gdelt_list_counter % SAVING_FREQ == 0):\n",
    "        sentiment_score_lr.to_csv(r\"Data/sentiment_score_lr.csv\")\n",
    "        sentiment_score_lstm.to_csv(r\"Data/sentiment_score_lstm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score_lr.to_csv(r\"Data/sentiment_score_lr.csv\")\n",
    "sentiment_score_lstm.to_csv(r\"Data/sentiment_score_lstm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Notebook: project_macro_model.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
